{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d0a5df0a-da1f-4170-8cbc-55eb74fc814d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alok Yadav\\anaconda3\\envs\\technocolab\\lib\\site-packages\\cupy\\_environment.py:216: UserWarning: CUDA path could not be detected. Set CUDA_PATH environment variable if CuPy fails to load.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from joblib import dump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8c27d65b-db3e-43e0-a001-b51e07e4caba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create your models here.\n",
    "data = pd.read_csv(\"C:/Users/Alok Yadav/Desktop/TechnoColab/Main_Project/mi_processed_data.csv\")\n",
    "X = data.drop(['status', 'isClosed'], axis=1)  # drop the target variable and the newly created binary target\n",
    "y_binary = data['isClosed']  # binary target\n",
    "y_multiclass = data['status']  # multiclass target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea903b14-6c5d-4456-919e-4241cc755522",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_multiclass_train, y_multiclass_test = train_test_split(X, y_multiclass, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ba4728b9-ff99-4028-9def-5530b8b17de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Extracting specific columns for different target variables:\n",
    "\n",
    "# yStatus_train = y_train.iloc[:,0]\n",
    "# yClosed_train = y_train.iloc[:,1]\n",
    "\n",
    "# yStatus_test = y_test.iloc[:,0]\n",
    "# yClosed_test = y_test.iloc[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0fc12db3-28db-4d35-adcb-6090056d718d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stdscaler = StandardScaler()\n",
    "# minmaxscaler = MinMaxScaler()\n",
    "# pca = PCA(n_components=0.9)\n",
    "# smote = SMOTE(random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ad9ea6e7-da23-4924-9c60-f03cf112ac64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGB = XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
    "# LGBM = LGBMClassifier()\n",
    "# NB = MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4baaf824-aab7-46f0-9f43-db7674a8e609",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_xgb = ImbPipeline([\n",
    "#     ('stdscaler', stdscaler),\n",
    "#     ('pca', pca),\n",
    "#     ('smote', smote),\n",
    "#     ('classifier', XGB)\n",
    "# ])\n",
    "\n",
    "# # Fit the model:\n",
    "# model_xgb.fit(X_train, yClosed_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "43768983-5fbd-4bcc-a3fc-cb5360690ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_lgbm = ImbPipeline([\n",
    "#     ('stdscaler', stdscaler),\n",
    "#     ('pca', pca),\n",
    "#     ('smote', smote),\n",
    "#     ('classifier', LGBM)\n",
    "# ])\n",
    "\n",
    "# # Fit the model:\n",
    "# model_lgbm.fit(X_train, yClosed_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4dddca6c-2472-4d36-abed-eb18a45497ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_nb = ImbPipeline([\n",
    "#     ('minmaxscaler', minmaxscaler),\n",
    "#     ('smote', smote),\n",
    "#     ('classifier', NB)\n",
    "# ])\n",
    "\n",
    "# # Fit the model:\n",
    "# model_nb.fit(X_train, yStatus_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "155ff34e-5a0e-4726-bbec-b90d5b8d55ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class CustomPipeline(BaseEstimator, ClassifierMixin):\n",
    "#     def __init__(self, binary_model_1, binary_model_2, multiclass_model, label_encoder=None):\n",
    "#         self.binary_model_1 = binary_model_1\n",
    "#         self.binary_model_2 = binary_model_2\n",
    "#         self.multiclass_model = multiclass_model\n",
    "#         self.label_encoder = label_encoder\n",
    "\n",
    "#     def fit(self, X, y_binary, y_multiclass):\n",
    "#         # Fit binary models on all data\n",
    "#         self.binary_model_1.fit(X, y_binary)\n",
    "#         self.binary_model_2.fit(X, y_binary)\n",
    "\n",
    "#         # Fit multiclass model on filtered subsets for both sets of conditions\n",
    "#         multiclass_indices = (self.binary_model_1.predict(X) == 1) & (self.binary_model_2.predict(X) == 1)\n",
    "#         acquired_closed_indices = (self.binary_model_1.predict(X) == 0) | (self.binary_model_2.predict(X) == 0)\n",
    "\n",
    "#         # Perhaps check the actual labels being passed\n",
    "#         print(\"Labels for multiclass training:\", y_multiclass[multiclass_indices | acquired_closed_indices])\n",
    "\n",
    "#         self.multiclass_model.fit(X[multiclass_indices | acquired_closed_indices], y_multiclass[multiclass_indices | acquired_closed_indices])\n",
    "#         return self\n",
    "\n",
    "#     def predict(self, X):\n",
    "#         pred_1 = self.binary_model_1.predict(X)\n",
    "#         pred_2 = self.binary_model_2.predict(X)\n",
    "#         final_predictions = np.empty(X.shape[0], dtype=int)  # ensure dtype is int for consistency\n",
    "\n",
    "#         for i in range(X.shape[0]):\n",
    "#             if pred_1[i] == 1 and pred_2[i] == 1:\n",
    "#                 # Predict 'operating' or 'IPO'\n",
    "#                 raw_pred = self.multiclass_model.predict(X[i:i+1])[0]\n",
    "#             else:\n",
    "#                 # Predict 'acquired' or 'closed'\n",
    "#                 raw_pred = self.multiclass_model.predict(X[i:i+1])[0]\n",
    "\n",
    "#             # Ensure predictions are handled as integers\n",
    "#             final_predictions[i] = int(raw_pred)  # Make sure predictions are integers\n",
    "\n",
    "#         return final_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f769f6e5-38f4-4698-8451-0e5f956fbe59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define all possible labels explicitly\n",
    "# all_labels = ['operating', 'ipo', 'acquired', 'closed']  # Add all potential labels\n",
    "# label_encoder = LabelEncoder()\n",
    "# label_encoder.fit(all_labels)\n",
    "\n",
    "# # Encode the multiclass labels using the label encoder\n",
    "# yStatus_train_encoded = label_encoder.transform(yStatus_train)\n",
    "# yStatus_test_encoded = label_encoder.transform(yStatus_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e13b1eb4-e09f-433b-86df-6fa48387df92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combined_pipeline = CustomPipeline(\n",
    "#     binary_model_1=model_xgb,\n",
    "#     binary_model_2=model_lgbm,\n",
    "#     multiclass_model=model_nb,\n",
    "#     label_encoder=label_encoder\n",
    "# )\n",
    "\n",
    "# # Fit the pipeline using the encoded multiclass labels and binary labels\n",
    "# combined_pipeline.fit(X_train, yClosed_train, yStatus_train_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7ec994de-bf18-49f5-8243-3e4de413f294",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define the base path for your models\n",
    "# base_path = 'C:/Users/Alok Yadav/Desktop/TechnoColab/Main_Project/WebApp/myproject/mlmodel/models/'\n",
    "\n",
    "# # Save the XGB model\n",
    "# dump(model_xgb, base_path + \"model_xgb.joblib\")\n",
    "\n",
    "# # Save the LGBM model\n",
    "# dump(model_lgbm, base_path + \"model_lgbm.joblib\")\n",
    "\n",
    "# # Save the NB model\n",
    "# dump(model_nb, base_path + \"model_nb.joblib\")\n",
    "\n",
    "# # Save the combined pipeline\n",
    "# dump(combined_pipeline, base_path + \"combined_pipeline.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f278c1c-5765-438f-b948-81a3e43957c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "827e4cad-820e-4039-b9fb-4fac1a81989c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "\n",
    "class CustomPipeline(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self):\n",
    "        # Initialize the binary and multiclass models\n",
    "        self.binary_model = XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)\n",
    "        self.model_open = MultinomialNB()\n",
    "        self.model_closed = MultinomialNB()\n",
    "        self.scaler = MinMaxScaler()  # Scaler for the input features\n",
    "        self.encoder = LabelEncoder()  # Encoder for the target labels\n",
    "        self.smote = SMOTE(random_state=42)  # SMOTE for handling class imbalance\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # Fit the scaler to the data\n",
    "        X_scaled = self.scaler.fit_transform(X)\n",
    "\n",
    "        # Encode and SMOTE transform the multiclass target\n",
    "        y_encoded = self.encoder.fit_transform(y)\n",
    "        X_smote, y_smote = self.smote.fit_resample(X_scaled, y_encoded)\n",
    "\n",
    "        # Fit the binary model\n",
    "        y_binary = (y_smote > 1)  # Regenerate binary targets: 'operating'/'ipo' as 1, 'acquired'/'closed' as 0\n",
    "        self.binary_model.fit(X_smote, y_binary)\n",
    "\n",
    "        # Prepare data for multiclass models based on binary target\n",
    "        open_mask = y_binary == 1\n",
    "        closed_mask = y_binary == 0\n",
    "\n",
    "        # Train the 'open' model\n",
    "        self.model_open.fit(X_smote[open_mask], y_smote[open_mask])\n",
    "\n",
    "        # Train the 'closed' model\n",
    "        self.model_closed.fit(X_smote[closed_mask], y_smote[closed_mask])\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        # Scale the features\n",
    "        X_scaled = self.scaler.transform(X)\n",
    "\n",
    "        # Predict using the binary model\n",
    "        binary_predictions = self.binary_model.predict(X_scaled)\n",
    "\n",
    "        # Determine the final predictions based on the binary outcome\n",
    "        final_predictions = []\n",
    "        for pred, features in zip(binary_predictions, X_scaled):\n",
    "            if pred == 1:\n",
    "                final_pred = self.encoder.inverse_transform(self.model_open.predict([features]))[0]\n",
    "            else:\n",
    "                final_pred = self.encoder.inverse_transform(self.model_closed.predict([features]))[0]\n",
    "            final_predictions.append(final_pred)\n",
    "\n",
    "        return final_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e068fafb-d685-4352-8a44-580d93fb86ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>CustomPipeline()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">CustomPipeline</label><div class=\"sk-toggleable__content\"><pre>CustomPipeline()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "CustomPipeline()"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create and fit the new pipeline with SMOTE\n",
    "combined_pipeline = CustomPipeline()\n",
    "combined_pipeline.fit(X_train, y_multiclass_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6a0043bf-7bad-4af8-be32-ce5c9a06b346",
   "metadata": {},
   "outputs": [],
   "source": [
    "smote_predictions = combined_pipeline.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5c9e5843-b2a0-4d00-9227-832437888282",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6500835840855901\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    acquired       0.82      0.75      0.78       883\n",
      "      closed       0.51      0.61      0.56       380\n",
      "         ipo       0.02      0.81      0.03       104\n",
      "   operating       1.00      0.64      0.78     13588\n",
      "\n",
      "    accuracy                           0.65     14955\n",
      "   macro avg       0.59      0.70      0.54     14955\n",
      "weighted avg       0.97      0.65      0.77     14955\n",
      "\n",
      "Confusion Matrix:\n",
      " [[ 661  221    0    1]\n",
      " [ 143  233    2    2]\n",
      " [   0    0   84   20]\n",
      " [   1    3 4840 8744]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_multiclass_test, smote_predictions))\n",
    "print(\"Classification Report:\\n\", classification_report(y_multiclass_test, smote_predictions))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_multiclass_test, smote_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "12947997-0421-4433-8f35-d89fa21be335",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['C:/Users/Alok Yadav/Desktop/TechnoColab/Main_Project/WebApp/myproject/mlmodel/models/combined_pipeline.joblib']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Define the base path for your models\n",
    "# base_path = 'C:/Users/Alok Yadav/Desktop/TechnoColab/Main_Project/WebApp/myproject/mlmodel/models/'\n",
    "\n",
    "# # Save the combined pipeline\n",
    "# dump(combined_pipeline, base_path + \"combined_pipeline.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60041a41-ad9f-4cfd-99fa-7be16e09277e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.naive_bayes import MultinomialNB\n",
    "# from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# # Update the numerical transformer to ensure non-negative values for Naive Bayes\n",
    "# numerical_transformer_nb = MinMaxScaler()\n",
    "\n",
    "# # Combine transformers into a preprocessor for Naive Bayes\n",
    "# preprocessor_nb = ColumnTransformer(\n",
    "#     transformers=[\n",
    "#         ('num', numerical_transformer_nb, numerical_cols),\n",
    "#         ('cat', categorical_transformer, categorical_cols)\n",
    "#     ])\n",
    "\n",
    "# # Define the Multiclass classifier in a pipeline\n",
    "# pipeline_nb = Pipeline(steps=[\n",
    "#     ('preprocessor', preprocessor_nb),\n",
    "#     ('classifier', MultinomialNB())\n",
    "# ])\n",
    "\n",
    "# # Fit the Multinomial Naive Bayes model\n",
    "# pipeline_nb.fit(X_train_multi, y_train_multi)\n",
    "\n",
    "# 'Multinomial Naive Bayes model is fitted and ready for predictions.'\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
